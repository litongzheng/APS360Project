{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "text_field = torchtext.data.Field(sequential=True,      # text sequence\n",
    "                                  tokenize=lambda x: x, # because are building a character-RNN\n",
    "                                  include_lengths=True, # to track the length of sequences, for batching\n",
    "                                  batch_first=True,\n",
    "                                  use_vocab=True)       # to turn each character into an integer index\n",
    "i = 0\n",
    "for line in open('sp3.txt'):\n",
    "        #print(line)\n",
    "        i = i+1\n",
    "        if i>=10:\n",
    "            break\n",
    "fields = [('line', text_field)]\n",
    "dataset = torchtext.data.TabularDataset(\"sp3.txt\", # name of the file\n",
    "                                        \"csv\",  # fields are separated by a tab\n",
    "                                        fields)\n",
    "split = [0.6, 0.2,0.2]\n",
    "train, valid, test =  dataset.split(split, stratified=False, strata_field=None, random_state=None)\n",
    "text_field.build_vocab(train)\n",
    "# print(text_field.vocab.stoi)\n",
    "# print(text_field.vocab.itos)\n",
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=32,\n",
    "                                           sort_key=lambda x: len(x.line), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                           repeat=False)                   # repeat the iterator for multiple epochs\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    lines = batch.line[0]\n",
    "    #print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##a naive model following the lecture notes with 1 hidden layer - GRU \n",
    "class CartmanLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_classes):\n",
    "        super(CartmanLSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.LSTM(vocab_size, hidden_size, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #set intiial hidden state and cell state\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        \n",
    "        #forward propogate\n",
    "        out, _ = self.rnn(x, (h0, c0))\n",
    "        \n",
    "        #passs output of last time step to classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, prime_str='I', predict_len=300, temperature=0.8):\n",
    "    \n",
    "    hidden = model.init_hidden()\n",
    "    prime_input = text_to_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = model(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0])\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = vocab_itos[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = text_to_tensor(predicted_char)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "def train(model, num_iters=2000, lr=0.004):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        # get training set\n",
    "        inp, target = random_training_set()\n",
    "        # cleanup\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        hidden = model.init_hidden()\n",
    "        output, _ = model(inp, hidden)\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if it % 200 == 199:\n",
    "            print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))\n",
    "            print(\"    \" + evaluate(model, ' ', 300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
